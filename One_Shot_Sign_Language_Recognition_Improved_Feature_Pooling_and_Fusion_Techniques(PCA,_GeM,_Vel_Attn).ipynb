{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPHmXGcBCvTrkQ3KXPz2cv3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rcharan05/UGP/blob/main/One_Shot_Sign_Language_Recognition_Improved_Feature_Pooling_and_Fusion_Techniques(PCA%2C_GeM%2C_Vel_Attn).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mounting the drive, importing all necessary libraries and loading all the data in the needed format"
      ],
      "metadata": {
        "id": "6vHtSD7ydPKG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCneDfm1042t",
        "outputId": "ef6f5066-53f4-4351-e137-574dd310e247"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.7/97.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install -q pose-format"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, numpy as np, pandas as pd\n",
        "from pose_format import Pose\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import normalize"
      ],
      "metadata": {
        "id": "CWR6PXL83fHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR       = \"/content/drive/MyDrive/UGP\"\n",
        "VIDEO_POSE_DIR = os.path.join(DATA_DIR, \"CISLR_v1.5-a_videos_poses\")\n",
        "I3D_PKL        = os.path.join(DATA_DIR, \"I3D_features.pkl\")\n",
        "PROTO_CSV      = os.path.join(DATA_DIR, \"prototype.csv\")\n",
        "TEST_CSV       = os.path.join(DATA_DIR, \"test.csv\")"
      ],
      "metadata": {
        "id": "TwB_FuQx3ig3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "proto_df = pd.read_csv(PROTO_CSV)\n",
        "proto_df[\"gloss\"] = proto_df[\"gloss\"].astype(str)\n",
        "test_df  = pd.read_csv(TEST_CSV)\n",
        "test_df[\"gloss\"]  = test_df[\"gloss\"].astype(str)\n",
        "y_tr, y_te = proto_df.gloss.tolist(), test_df.gloss.tolist()"
      ],
      "metadata": {
        "id": "vS5B_5_C3k9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i3d_df = pd.read_pickle(I3D_PKL)\n",
        "i3d_dict = {row[\"id\"]: np.array(row[\"I3D_features\"], dtype=np.float32)\n",
        "            for _, row in i3d_df.iterrows()}"
      ],
      "metadata": {
        "id": "CuLHMYt-3nYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defined the evaluation piece of code to evaluate each model"
      ],
      "metadata": {
        "id": "aD3GRdYDdiWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def topk_from_S(S, y_tr, y_te, k):\n",
        "    ranks = np.argsort(-S, axis=1)\n",
        "    return np.mean([ y_te[i] in [y_tr[j] for j in ranks[i,:k]]\n",
        "                     for i in range(len(y_te)) ]) * 100"
      ],
      "metadata": {
        "id": "binoaLk_3phP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#OPTION 1: Improved I3D Feature Extraction via GeM Pooling"
      ],
      "metadata": {
        "id": "f7w-DO3_doZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gem_pooling(arr, p=2.0):\n",
        "    # arr: shape (1024, S)\n",
        "    return (np.mean(np.power(arr + 1e-6, p), axis=1) + 1e-12) ** (1.0/p)"
      ],
      "metadata": {
        "id": "VvgcpH5i3w2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def improved_i3d_feat_GeM(uid, p=2.0):\n",
        "    # Extract I3D features: (1024, S)\n",
        "    arr = i3d_dict[uid].squeeze((0,3,4))\n",
        "    f = gem_pooling(arr, p)\n",
        "    f = np.sign(f) * np.sqrt(np.abs(f) + 1e-8)\n",
        "    return f / np.linalg.norm(f)"
      ],
      "metadata": {
        "id": "FcqnmWm73z8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build features with Option 1 for training and testing\n",
        "X_imp_GeM = np.stack([improved_i3d_feat_GeM(u) for u in proto_df.uid])\n",
        "X_imp_te_GeM = np.stack([improved_i3d_feat_GeM(u) for u in test_df.uid])\n",
        "X_imp_GeM_n = normalize(X_imp_GeM, axis=1)\n",
        "X_imp_te_GeM_n = normalize(X_imp_te_GeM, axis=1)"
      ],
      "metadata": {
        "id": "GKZ3Dgwv34Kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For fusion, use PCA-whiten on GeM features if desired.\n",
        "pca = PCA(whiten=True, n_components=512).fit(X_imp_GeM)\n",
        "X_pw = pca.transform(X_imp_GeM)\n",
        "X_pw_te = pca.transform(X_imp_te_GeM)\n",
        "S_geM = normalize(X_pw_te, axis=1).dot(normalize(X_pw, axis=1).T)"
      ],
      "metadata": {
        "id": "rSV8q82o39vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Option 1: GeM Pooling on I3D Features ===\")\n",
        "for k in (1, 5, 10):\n",
        "    print(f\"Top-{k}: {topk_from_S(S_geM, y_tr, y_te, k):.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHR2rPdB4A4k",
        "outputId": "1310a2da-44a0-4e00-f9bf-bb8a591c7295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Option 1: GeM Pooling on I3D Features ===\n",
            "Top-1: 19.21%\n",
            "Top-5: 24.60%\n",
            "Top-10: 27.92%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experimented with different hyper-parameters, and the best is used below. This is the best model over-all with the best top-1 accuracy."
      ],
      "metadata": {
        "id": "ba8XMTT9dzPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For fusion, use PCA-whiten on GeM features if desired.\n",
        "pca = PCA(whiten=True, n_components=1024).fit(X_imp_GeM)\n",
        "X_pw = pca.transform(X_imp_GeM)\n",
        "X_pw_te = pca.transform(X_imp_te_GeM)\n",
        "S_geM = normalize(X_pw_te, axis=1).dot(normalize(X_pw, axis=1).T)\n",
        "print(\"=== Option 1: GeM Pooling on I3D Features ===\")\n",
        "for k in (1, 5, 10):\n",
        "    print(f\"Top-{k}: {topk_from_S(S_geM, y_tr, y_te, k):.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUutf5d9HHGR",
        "outputId": "2eae83d9-0a4a-4f46-e61d-7fe33362adb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Option 1: GeM Pooling on I3D Features ===\n",
            "Top-1: 19.69%\n",
            "Top-5: 24.64%\n",
            "Top-10: 27.22%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#OPTION 2: Attention-based Pooling for Pose Velocity Features"
      ],
      "metadata": {
        "id": "5giDfa3Wd9KK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "CwRMDush4MaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention based pooling module"
      ],
      "metadata": {
        "id": "696XUe9me4JN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionPooling(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim=128):\n",
        "        super(AttentionPooling, self).__init__()\n",
        "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.fc2 = nn.Linear(hidden_dim, 1)  # attention score per frame\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (T, D) where T = number of frames, D = feature dimension per frame\n",
        "        # Compute attention scores\n",
        "        attn = self.fc2(self.tanh(self.fc1(x)))  # (T,1)\n",
        "        attn = torch.softmax(attn, dim=0)         # (T,1) weights sum to 1\n",
        "        pooled = torch.sum(attn * x, dim=0)         # (D,)\n",
        "        return pooled"
      ],
      "metadata": {
        "id": "FtY8rz8B4Oj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Attention-based Pose Velocity Feature Extraction"
      ],
      "metadata": {
        "id": "pmbyNyY6e91P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pose_velocity_feat_attention(uid):\n",
        "    \"\"\"\n",
        "    Compute velocity features on face+hands with attention-based pooling.\n",
        "    - Extract face (indices 33:33+468) and hands (indices 501:501+21 and 522:522+21).\n",
        "    - Compute framewise differences and then apply attention pooling.\n",
        "    Final dimension will be the same as the per-frame feature dimension.\n",
        "    \"\"\"\n",
        "    buf = open(os.path.join(VIDEO_POSE_DIR, f\"{uid}.pose\"), \"rb\").read()\n",
        "    p = Pose.read(buf)\n",
        "    coords = p.body.data.squeeze(1)[...,:2]  # (T,576,2)\n",
        "    if coords.shape[0] < 2:\n",
        "        feat = np.zeros(510*2)  # fallback for very short sequences\n",
        "        return feat / (np.linalg.norm(feat) + 1e-6)\n",
        "    # face: indices 33:33+468, left-hand: 501:501+21, right-hand: 522:522+21\n",
        "    face = coords[:,33:33+468]\n",
        "    lh   = coords[:,501:501+21]\n",
        "    rh   = coords[:,522:522+21]\n",
        "    pts  = np.concatenate([face, lh, rh], axis=1)  # (T,510,2)\n",
        "    # Compute frame-wise differences (velocity)\n",
        "    diffs = np.linalg.norm(pts[1:] - pts[:-1], axis=2)  # (T-1, 510)\n",
        "    frame_feats = diffs  # using raw differences; you can also add other stats per frame\n",
        "    # Convert to torch tensor and apply attention pooling\n",
        "    frame_feats = torch.from_numpy(frame_feats).float()  # shape: (T-1, 510)\n",
        "    attn_pool = AttentionPooling(in_dim=frame_feats.shape[1])\n",
        "    with torch.no_grad():\n",
        "        pooled = attn_pool(frame_feats)  # (510,)\n",
        "    pooled_np = pooled.numpy()\n",
        "    pooled_np = np.sign(pooled_np) * np.sqrt(np.abs(pooled_np) + 1e-8)\n",
        "    return pooled_np / (np.linalg.norm(pooled_np) + 1e-6)"
      ],
      "metadata": {
        "id": "seyffzzO4R1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute Attention-based Features in Parallel"
      ],
      "metadata": {
        "id": "mfhNnoUyfBPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent.futures\n",
        "\n",
        "def compute_att_feat(uid):\n",
        "    # This function wraps your existing attention pooling feature computation.\n",
        "    return pose_velocity_feat_attention(uid)\n",
        "\n",
        "# Use ThreadPoolExecutor to compute training velocity features in parallel\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    X_vel_att_list = list(executor.map(compute_att_feat, proto_df.uid))\n",
        "X_vel_att = np.stack(X_vel_att_list)\n",
        "\n",
        "# Use ThreadPoolExecutor to compute test velocity features in parallel\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    X_vel_att_te_list = list(executor.map(compute_att_feat, test_df.uid))\n",
        "X_vel_att_te = np.stack(X_vel_att_te_list)\n",
        "\n",
        "# L2-normalize the features as before\n",
        "from sklearn.preprocessing import normalize\n",
        "X_vel_att_n = normalize(X_vel_att, axis=1)\n",
        "X_vel_att_te_n = normalize(X_vel_att_te, axis=1)"
      ],
      "metadata": {
        "id": "J_q8nWIY4WK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Score Computation and Fusion (Attention-based)"
      ],
      "metadata": {
        "id": "tVK3JYBwfG2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the same PCA-whitened I3D (X_pw, X_pw_te) from above.\n",
        "S_att = normalize(X_pw_te, axis=1).dot(normalize(X_pw, axis=1).T)  # I3D sim\n",
        "S_att_vel = normalize(X_vel_att_te_n, axis=1).dot(normalize(X_vel_att_n, axis=1).T)  # velocity sim\n",
        "# Fuse scores with best fusion weights (for demonstration, let α=0.7, β=0.3)\n",
        "S_fuse_att = 0.7 * S_att + 0.3 * S_att_vel"
      ],
      "metadata": {
        "id": "Lgkn5tx64Ytp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Option 2: Attention-based Pooling for Velocity Features ===\")\n",
        "for k in (1, 5, 10):\n",
        "    print(f\"Top-{k}: {topk_from_S(S_fuse_att, y_tr, y_te, k):.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQS0wgEB4bGc",
        "outputId": "c5971b52-5473-46c7-c455-fb183fd48db8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Option 2: Attention-based Pooling for Velocity Features ===\n",
            "Top-1: 18.86%\n",
            "Top-5: 24.29%\n",
            "Top-10: 27.22%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baseline Pose Velocity Feature Extraction (Mean + Max)"
      ],
      "metadata": {
        "id": "wyrfajSZfLEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pose_velocity_feat(uid):\n",
        "    \"\"\"\n",
        "    Load the pose file for uid (expected shape: (T,1,576,3)), squeeze to (T,576,3),\n",
        "    extract face and hands landmarks, compute per‐frame velocity (L2 norm of differences),\n",
        "    then aggregate by computing the mean and maximum across time.\n",
        "    Finally apply signed‑sqrt and L2‑normalization.\n",
        "\n",
        "    - Face landmarks: indices 33 to 33+468 (468 points)\n",
        "    - Left hand: indices 501 to 501+21 (21 points)\n",
        "    - Right hand: indices 522 to 522+21 (21 points)\n",
        "    Combined, this gives 510 points.\n",
        "    The feature dimension is then 2*510 = 1020.\n",
        "    \"\"\"\n",
        "    # Read the pose file\n",
        "    buf = open(os.path.join(VIDEO_POSE_DIR, f\"{uid}.pose\"), \"rb\").read()\n",
        "    p = Pose.read(buf)\n",
        "    # Get coordinates: (T,576,3) and take only the first 2 dimensions (x,y)\n",
        "    coords = p.body.data.squeeze(1)[...,:2]  # (T,576,2)\n",
        "\n",
        "    # If the sequence is too short, use a zero vector as fallback\n",
        "    if coords.shape[0] < 2:\n",
        "        feat = np.zeros(510 * 2)\n",
        "        return feat / (np.linalg.norm(feat) + 1e-6)\n",
        "\n",
        "    # Extract face and hands (use indices as per MediaPipe ordering)\n",
        "    # Face: indices 33:33+468, Left Hand: indices 501:501+21, Right Hand: indices 522:522+21\n",
        "    face = coords[:, 33:33+468]      # (T,468,2)\n",
        "    lh   = coords[:, 501:501+21]      # (T,21,2)\n",
        "    rh   = coords[:, 522:522+21]      # (T,21,2)\n",
        "    pts  = np.concatenate([face, lh, rh], axis=1)  # (T,510,2)\n",
        "\n",
        "    # Compute frame-wise differences (velocities)\n",
        "    diffs = np.linalg.norm(pts[1:] - pts[:-1], axis=2)  # (T-1,510)\n",
        "    mean_sp = diffs.mean(axis=0)  # (510,)\n",
        "    max_sp  = diffs.max(axis=0)   # (510,)\n",
        "    feat = np.concatenate([mean_sp, max_sp])  # (1020,)\n",
        "\n",
        "    # Apply signed sqrt and L2 normalization\n",
        "    feat = np.sign(feat) * np.sqrt(np.abs(feat) + 1e-8)\n",
        "    norm = np.linalg.norm(feat)\n",
        "    return feat / norm if norm > 0 else feat\n"
      ],
      "metadata": {
        "id": "u5ntMMH78Ci2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_vel    = np.stack([pose_velocity_feat(u) for u in proto_df.uid])\n",
        "X_vel_te = np.stack([pose_velocity_feat(u) for u in test_df.uid])\n",
        "\n",
        "# L2-normalize these velocity features\n",
        "X_vel_n    = normalize(X_vel, axis=1)\n",
        "X_vel_te_n = normalize(X_vel_te, axis=1)"
      ],
      "metadata": {
        "id": "gdYONnmP7Aq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OPTION 3: Nonlinear Fusion via a Small MLP (Late Fusion)\n"
      ],
      "metadata": {
        "id": "1ANzSZhbfRcV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nonlinear MLP Fusion of Similarity Scores"
      ],
      "metadata": {
        "id": "1dBIy5HsfOwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class NonlinearFusionMLP(nn.Module):\n",
        "    def __init__(self, sim_dim):\n",
        "        \"\"\"\n",
        "        sim_dim: dimension of similarity vector (could be the number of training samples)\n",
        "        Here we assume we are fusing the flattened scores from two modalities.\n",
        "        \"\"\"\n",
        "        super(NonlinearFusionMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 16)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(16, 1)\n",
        "\n",
        "    def forward(self, s1, s2):\n",
        "        # s1, s2: similarity scores (scalars) or stacked along dim=1\n",
        "        # For each query-gallery pair, fuse the two scores.\n",
        "        x = torch.cat([s1, s2], dim=1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        out = self.fc2(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "EJ__fJjY5TcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "S_i3d = normalize(X_pw_te, axis=1).dot(normalize(X_pw, axis=1).T)   # I3D similarity from PCA-whitened features\n",
        "S_vel = normalize(X_vel_te_n, axis=1).dot(normalize(X_vel_n, axis=1).T)  # Velocity similarity (using baseline pose_velocity_feat)"
      ],
      "metadata": {
        "id": "zjmB0DLh5XL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fusion_model = NonlinearFusionMLP(sim_dim=2).cuda()\n",
        "\n",
        "# Prepare the scores as torch tensors\n",
        "S_i3d_t = torch.from_numpy(S_i3d).float().cuda()\n",
        "S_vel_t = torch.from_numpy(S_vel).float().cuda()"
      ],
      "metadata": {
        "id": "urAPXU-m5a3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    # Concatenate similarity scores then reshape to (-1,2)\n",
        "    fuse_input = torch.cat([S_i3d_t.unsqueeze(2), S_vel_t.unsqueeze(2)], dim=2).view(-1, 2)\n",
        "    # Split the input into two tensors along the columns\n",
        "    s1 = fuse_input[:, 0].unsqueeze(1)  # (N, 1)\n",
        "    s2 = fuse_input[:, 1].unsqueeze(1)  # (N, 1)\n",
        "    fused_scores = fusion_model(s1, s2)\n",
        "    S_fused_mlp = fused_scores.view(S_i3d.shape)\n",
        "\n",
        "S_fused_mlp = S_fused_mlp.cpu().numpy()"
      ],
      "metadata": {
        "id": "S-H9eBP95fv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Option 3: Nonlinear Fusion with MLP ===\")\n",
        "for k in (1, 5, 10):\n",
        "    print(f\"Top-{k}: {topk_from_S(S_fused_mlp, y_tr, y_te, k):.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqM0h_nY5idW",
        "outputId": "7c023599-02a3-4b80-b0b3-f1b853ae869e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Option 3: Nonlinear Fusion with MLP ===\n",
            "Top-1: 0.00%\n",
            "Top-5: 0.09%\n",
            "Top-10: 0.09%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter Search for GeM Pooling (for general use)"
      ],
      "metadata": {
        "id": "e3Fvo5SofhY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter search for GeM pooling\n",
        "p_values = [1.0, 2.0, 3.0, 4.0, 5.0]\n",
        "best_p = None\n",
        "best_acc = 0.0\n",
        "\n",
        "for p in p_values:\n",
        "    X_imp_GeM = np.stack([improved_i3d_feat_GeM(u, p=p) for u in proto_df.uid])\n",
        "    X_imp_te_GeM = np.stack([improved_i3d_feat_GeM(u, p=p) for u in test_df.uid])\n",
        "    # You can choose to use PCA-whitened features on top\n",
        "    pca = PCA(whiten=True, n_components=512).fit(X_imp_GeM)\n",
        "    X_pw = pca.transform(X_imp_GeM)\n",
        "    X_pw_te = pca.transform(X_imp_te_GeM)\n",
        "    S_temp = normalize(X_pw_te, axis=1).dot(normalize(X_pw, axis=1).T)\n",
        "    acc1 = topk_from_S(S_temp, y_tr, y_te, 1)\n",
        "    print(f\"p={p} Top-1: {acc1:.2f}%\")\n",
        "    if acc1 > best_acc:\n",
        "        best_acc, best_p = acc1, p\n",
        "\n",
        "print(f\"Best GeM pooling p: {best_p} with Top-1: {best_acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dikmeNGV-7Gz",
        "outputId": "ec8f4c16-1b5b-4102-a485-99980ff0e560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p=1.0 Top-1: 19.26%\n",
            "p=2.0 Top-1: 19.30%\n",
            "p=3.0 Top-1: 18.91%\n",
            "p=4.0 Top-1: 18.86%\n",
            "p=5.0 Top-1: 18.86%\n",
            "Best GeM pooling p: 2.0 with Top-1: 19.30%\n"
          ]
        }
      ]
    }
  ]
}